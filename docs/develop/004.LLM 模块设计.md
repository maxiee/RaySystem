在 RaySytem 的后端工程中，我希望引入一个新模块——LLM 模块。

LLM 模块用于访问 LLM 模型，能够支持对接不同的 LLM 服务。

我希望将模块创建在 `raysystem/module/llm` 目录下。

对于模块的设计，我有一套标准化的设计方案，具体参见：`docs/develop/003.后端工程的标准结构.md`。

我规划的功能如下：

## 配置 LLM 服务

在 `RaySystemConfig.yaml` 中配置 LLM 服务的地址和密钥等信息。

我使用 openai 这个 SDK，目前接入的服务都兼容这个 SDK。因此在配置中，需要用户填入：

- `llm_service`: LLM 服务的地址
- `llm_key`: LLM 服务的密钥
- `llm_model`: LLM 模型的名称

## AsyncIO 异步 OpenAI SDK 调用

RaySystem 的后段工程是一个使用 AsyncIO 的 FastAPI 项目，因此需要使用异步的方式调用 OpenAI 的 SDK。

不能阻塞 FastAPI 的事件循环。

## FastAPI 封装 OpenAI SDK

我通过 FastAPI 向前端提供 API 接口，因此需要封装 OpenAI 的 SDK，提供一个可供前端调用的接口。

同时需要注意，前端的 Client 调用时通过 OpenAPI 协议生成器自动生成的 Dart 代码。

因此需要保证，我们提供的 API 接口，能够被 OpenAPI 协议生成器正确解析。

## 支持添加中间件

我希望在调用 OpenAI 的 SDK 时，能够支持添加中间件。

例如，允许添加中间件，供我获取到聊天会话，使我能够将对话保存到数据库当中，这样我可以不断积累语料，用于未来我自己的模型训练中。

再例如，允许添加中间件，供我统计 token 的使用情况。

再例如，允许添加中间件，用于未来实现 Agent、记忆等功能。

我们这次不必实现这些中间件，只需要提供一个接口，供我在未来实现中间件时使用。

